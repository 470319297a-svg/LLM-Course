"""
åŸºäºQwen-2.5çš„RAGç³»ç»Ÿ for RTCA DO-160Gæ ‡å‡†æ–‡æ¡£
æ”¯æŒï¼šå¤šè½®å¯¹è¯ã€é•¿ä¸Šä¸‹æ–‡ã€å¼•ç”¨æ˜¾ç¤ºã€æ‹’ç»ä¸ç¡®å®šå›ç­”
ä½œè€…ï¼šAIåŠ©æ‰‹
ç‰ˆæœ¬ï¼š1.0
"""

# ==================== 1. ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£… ====================
# requirements.txt
"""
torch>=2.0.0
transformers>=4.35.0
sentence-transformers>=2.2.0
faiss-cpu>=1.7.0
langchain>=0.0.340
gradio>=4.0.0
streamlit>=1.28.0
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.0.0
chromadb>=0.4.0
accelerate>=0.24.0
peft>=0.6.0
evaluate>=0.4.0
rouge-score>=0.1.2
pdfplumber>=0.10.0
python-docx>=1.1.0
rank_bm25>=0.2.2
tiktoken>=0.5.0
"""

# ==================== 2. é…ç½®ç®¡ç† ====================
import yaml
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
from enum import Enum
import os
import re
from typing import List, Tuple
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
import torch
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from typing import List, Dict
from rank_bm25 import BM25Okapi
from transformers import AutoTokenizer
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel, LoraConfig, get_peft_model
from typing import List, Dict, Tuple
import datasets
from transformers import TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
import gradio as gr
import time
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
from sklearn.metrics import precision_recall_fscore_support
import evaluate


class RetrievalStrategy(Enum):
    DENSE = "dense"
    SPARSE = "sparse"
    HYBRID = "hybrid"
    RERANK = "rerank"


@dataclass
class RAGConfig:
    """RAGç³»ç»Ÿé…ç½®"""
    # æ¨¡å‹é…ç½®
    model_name: str = "Qwen/Qwen2.5-7B-Instruct"
    embedding_model: str = "BAAI/bge-large-zh-v1.5"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    # æ£€ç´¢é…ç½®
    retrieval_strategy: RetrievalStrategy = RetrievalStrategy.HYBRID
    top_k: int = 5
    rerank_top_n: int = 3
    chunk_size: int = 512
    chunk_overlap: int = 50

    # ç”Ÿæˆé…ç½®
    max_new_tokens: int = 1024
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.1

    # å¾®è°ƒé…ç½®
    use_lora: bool = False
    lora_rank: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1

    # è·¯å¾„é…ç½®
    knowledge_base_path: str = "./data/knowledge_base"
    vector_db_path: str = "./data/vector_db"
    fine_tune_data_path: str = "./data/fine_tune"

    def save(self, path: str):
        with open(path, 'w') as f:
            json.dump(self.__dict__, f, indent=2)

    @classmethod
    def load(cls, path: str):
        with open(path, 'r') as f:
            data = json.load(f)
        return cls(**data)


# ==================== 3. æ–‡æ¡£å¤„ç†æ¨¡å— ====================


class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", "ã€", " "]
        )

    def load_pdf(self, file_path: str) -> List[Dict]:
        """åŠ è½½PDFæ–‡æ¡£"""
        documents = []
        with pdfplumber.open(file_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                text = page.extract_text()
                if text:
                    # æå–ç« èŠ‚ä¿¡æ¯
                    chapter_match = re.search(r'ç¬¬(\d+)ç« \s+(.+)', text[:100])
                    chapter_info = {
                        'chapter': chapter_match.group(1) if chapter_match else str(page_num),
                        'title': chapter_match.group(2) if chapter_match else f"ç¬¬{page_num}é¡µ",
                        'page': page_num
                    }

                    documents.append({
                        'content': text,
                        'metadata': {
                            'source': os.path.basename(file_path),
                            'page': page_num,
                            'chapter_info': chapter_info
                        }
                    })
        return documents

    def chunk_documents(self, documents: List[Dict]) -> List[Dict]:
        """æ–‡æ¡£åˆ†å—"""
        chunks = []
        for doc in documents:
            text_chunks = self.text_splitter.split_text(doc['content'])
            for i, chunk in enumerate(text_chunks):
                chunks.append({
                    'content': chunk,
                    'metadata': {
                        **doc['metadata'],
                        'chunk_id': i,
                        'start_char': i * self.config.chunk_size
                    }
                })
        return chunks

    def process_directory(self, dir_path: str) -> List[Dict]:
        """å¤„ç†æ•´ä¸ªç›®å½•çš„æ–‡æ¡£"""
        all_chunks = []
        for root, _, files in os.walk(dir_path):
            for file in files:
                if file.endswith('.pdf'):
                    file_path = os.path.join(root, file)
                    documents = self.load_pdf(file_path)
                    chunks = self.chunk_documents(documents)
                    all_chunks.extend(chunks)

        # ä¿å­˜å¤„ç†åçš„æ–‡æ¡£
        self.save_chunks(all_chunks, os.path.join(
            self.config.knowledge_base_path, "processed_chunks.json"))
        return all_chunks

    @staticmethod
    def save_chunks(chunks: List[Dict], output_path: str):
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)


# ==================== 4. å‘é‡æ•°æ®åº“æ¨¡å— ====================


class VectorStoreManager:
    """å‘é‡æ•°æ®åº“ç®¡ç†å™¨"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=config.embedding_model,
            model_kwargs={'device': config.device},
            encode_kwargs={'normalize_embeddings': True}
        )
        self.vector_store = None

    def create_vector_store(self, chunks: List[Dict]) -> FAISS:
        """åˆ›å»ºå‘é‡æ•°æ®åº“"""
        texts = [chunk['content'] for chunk in chunks]
        metadatas = [chunk['metadata'] for chunk in chunks]

        self.vector_store = FAISS.from_texts(
            texts=texts,
            embedding=self.embedding_model,
            metadatas=metadatas
        )

        # ä¿å­˜å‘é‡æ•°æ®åº“
        self.vector_store.save_local(self.config.vector_db_path)
        return self.vector_store

    def load_vector_store(self) -> FAISS:
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        self.vector_store = FAISS.load_local(
            self.config.vector_db_path,
            self.embedding_model,
            allow_dangerous_deserialization=True
        )
        return self.vector_store

    def update_vector_store(self, new_chunks: List[Dict]):
        """æ›´æ–°å‘é‡æ•°æ®åº“"""
        if self.vector_store is None:
            self.load_vector_store()

        texts = [chunk['content'] for chunk in new_chunks]
        metadatas = [chunk['metadata'] for chunk in new_chunks]

        self.vector_store.add_texts(texts, metadatas)
        self.vector_store.save_local(self.config.vector_db_path)


# ==================== 5. æ£€ç´¢æ¨¡å— ====================


class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""

    def __init__(self, vector_store: FAISS, chunks: List[Dict], config: RAGConfig):
        self.vector_store = vector_store
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(config.embedding_model)

        # åˆå§‹åŒ–BM25
        self.chunks = chunks
        self.chunk_texts = [chunk['content'] for chunk in chunks]
        self.bm25 = BM25Okapi([self.tokenize(text)
                              for text in self.chunk_texts])

        # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
        self.rerank_model = None
        if config.retrieval_strategy == RetrievalStrategy.RERANK:
            self.init_rerank_model()

    def tokenize(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯"""
        return self.tokenizer.tokenize(text)

    def init_rerank_model(self):
        """åˆå§‹åŒ–é‡æ’åºæ¨¡å‹"""
        from sentence_transformers import CrossEncoder
        self.rerank_model = CrossEncoder('BAAI/bge-reranker-large')

    def dense_retrieve(self, query: str, top_k: int) -> List[Dict]:
        """ç¨ å¯†æ£€ç´¢"""
        docs = self.vector_store.similarity_search_with_score(query, k=top_k)
        results = []
        for doc, score in docs:
            results.append({
                'content': doc.page_content,
                'metadata': doc.metadata,
                'score': float(score),
                'retrieval_type': 'dense'
            })
        return results

    def sparse_retrieve(self, query: str, top_k: int) -> List[Dict]:
        """ç¨€ç–æ£€ç´¢ï¼ˆBM25ï¼‰"""
        tokenized_query = self.tokenize(query)
        scores = self.bm25.get_scores(tokenized_query)

        # è·å–top_kç»“æœ
        top_indices = np.argsort(scores)[::-1][:top_k]

        results = []
        for idx in top_indices:
            results.append({
                'content': self.chunk_texts[idx],
                'metadata': self.chunks[idx]['metadata'],
                'score': float(scores[idx]),
                'retrieval_type': 'sparse'
            })
        return results

    def hybrid_retrieve(self, query: str, top_k: int, alpha: float = 0.5) -> List[Dict]:
        """æ··åˆæ£€ç´¢"""
        dense_results = self.dense_retrieve(query, top_k * 2)
        sparse_results = self.sparse_retrieve(query, top_k * 2)

        # åˆå¹¶ç»“æœå¹¶å»é‡
        all_results = {}
        for result in dense_results + sparse_results:
            content = result['content']
            if content not in all_results:
                all_results[content] = {
                    'content': content,
                    'metadata': result['metadata'],
                    'dense_score': 0.0,
                    'sparse_score': 0.0,
                    'combined_score': 0.0
                }

            if result['retrieval_type'] == 'dense':
                all_results[content]['dense_score'] = result['score']
            else:
                all_results[content]['sparse_score'] = result['score']

        # å½’ä¸€åŒ–åˆ†æ•°å¹¶è®¡ç®—ç»¼åˆåˆ†æ•°
        max_dense = max(r['dense_score'] for r in all_results.values()) or 1
        max_sparse = max(r['sparse_score'] for r in all_results.values()) or 1

        for content in all_results:
            all_results[content]['dense_score_norm'] = all_results[content]['dense_score'] / max_dense
            all_results[content]['sparse_score_norm'] = all_results[content]['sparse_score'] / max_sparse
            all_results[content]['combined_score'] = (
                alpha * all_results[content]['dense_score_norm'] +
                (1 - alpha) * all_results[content]['sparse_score_norm']
            )

        # æŒ‰ç»¼åˆåˆ†æ•°æ’åº
        sorted_results = sorted(all_results.values(),
                                key=lambda x: x['combined_score'],
                                reverse=True)[:top_k]

        return [{
            'content': r['content'],
            'metadata': r['metadata'],
            'score': r['combined_score'],
            'retrieval_type': 'hybrid'
        } for r in sorted_results]

    def retrieve_with_rerank(self, query: str, top_k: int) -> List[Dict]:
        """å¸¦é‡æ’åºçš„æ£€ç´¢"""
        # ç¬¬ä¸€é˜¶æ®µï¼šæ··åˆæ£€ç´¢è·å–æ›´å¤šå€™é€‰
        candidate_results = self.hybrid_retrieve(query, top_k * 3)

        # ç¬¬äºŒé˜¶æ®µï¼šé‡æ’åº
        if self.rerank_model:
            pairs = [(query, r['content']) for r in candidate_results]
            rerank_scores = self.rerank_model.predict(pairs)

            for result, score in zip(candidate_results, rerank_scores):
                result['rerank_score'] = float(score)

            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            candidate_results.sort(
                key=lambda x: x['rerank_score'], reverse=True)

        return candidate_results[:top_k]

    def retrieve(self, query: str, top_k: Optional[int] = None) -> List[Dict]:
        """ä¸»æ£€ç´¢æ–¹æ³•"""
        if top_k is None:
            top_k = self.config.top_k

        strategy = self.config.retrieval_strategy

        if strategy == RetrievalStrategy.DENSE:
            return self.dense_retrieve(query, top_k)
        elif strategy == RetrievalStrategy.SPARSE:
            return self.sparse_retrieve(query, top_k)
        elif strategy == RetrievalStrategy.HYBRID:
            return self.hybrid_retrieve(query, top_k)
        elif strategy == RetrievalStrategy.RERANK:
            return self.retrieve_with_rerank(query, top_k)
        else:
            raise ValueError(f"Unknown retrieval strategy: {strategy}")


# ==================== 6. æ¨¡å‹ç®¡ç†æ¨¡å— ====================


class QwenModelManager:
    """Qwenæ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.lora_config = None

    def load_base_model(self):
        """åŠ è½½åŸºç¡€æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.config.model_name}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            torch_dtype=torch.float16 if self.config.device == "cuda" else torch.float32,
            device_map="auto" if self.config.device == "cuda" else None,
            trust_remote_code=True
        )

        if self.config.use_lora:
            self.apply_lora()

        # åˆ›å»ºtext generation pipeline
        self.pipeline = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            device=0 if self.config.device == "cuda" else -1
        )

        return self.model, self.tokenizer

    def apply_lora(self):
        """åº”ç”¨LoRAé€‚é…å™¨"""
        self.lora_config = LoraConfig(
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=["q_proj", "v_proj", "k_proj",
                            "o_proj", "gate_proj", "up_proj", "down_proj"],
            bias="none",
            task_type="CAUSAL_LM"
        )

        self.model = get_peft_model(self.model, self.lora_config)
        print("LoRAé€‚é…å™¨å·²åŠ è½½")

    def load_lora_adapter(self, adapter_path: str):
        """åŠ è½½é¢„è®­ç»ƒçš„LoRAé€‚é…å™¨"""
        if self.model is None:
            self.load_base_model()

        self.model = PeftModel.from_pretrained(self.model, adapter_path)
        self.model = self.model.merge_and_unload()
        print(f"LoRAé€‚é…å™¨å·²ä» {adapter_path} åŠ è½½")

    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if self.pipeline is None:
            self.load_base_model()

        # åˆå¹¶ç”Ÿæˆå‚æ•°
        gen_kwargs = {
            "max_new_tokens": self.config.max_new_tokens,
            "temperature": self.config.temperature,
            "top_p": self.config.top_p,
            "repetition_penalty": self.config.repetition_penalty,
            "do_sample": True,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id
        }
        gen_kwargs.update(kwargs)

        # ç”Ÿæˆ
        outputs = self.pipeline(prompt, **gen_kwargs)
        return outputs[0]['generated_text'][len(prompt):]

    def chat(self, messages: List[Dict], **kwargs) -> str:
        """å¯¹è¯ç”Ÿæˆ"""
        if self.pipeline is None:
            self.load_base_model()

        # æ„å»ºå¯¹è¯æ ¼å¼
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        return self.generate(text, **kwargs)


# ==================== 7. RAGç³»ç»Ÿæ ¸å¿ƒ ====================


class RAGSystem:
    """RAGç³»ç»Ÿæ ¸å¿ƒ"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.model_manager = QwenModelManager(config)
        self.retriever = None
        self.conversation_history = []

        # åŠ è½½ç³»ç»Ÿprompt
        self.system_prompt = self.load_system_prompt()

        # ä¸ç¡®å®šæ€§æ£€æµ‹å…³é”®è¯
        self.uncertain_keywords = [
            "æˆ‘ä¸ç¡®å®š", "æˆ‘ä¸çŸ¥é“", "æ— æ³•ç¡®å®š", "æ²¡æœ‰æ‰¾åˆ°", "æœªæåŠ",
            "å¯èƒ½", "å¤§æ¦‚", "æˆ–è®¸", "ä¼¼ä¹", "åº”è¯¥"
        ]

    def load_system_prompt(self) -> str:
        """åŠ è½½ç³»ç»Ÿprompt"""
        return """ä½ æ˜¯ä¸€ä¸ªèˆªç©ºæ ‡å‡†RTCA DO-160Gçš„ä¸“å®¶åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ã€‚
        å›ç­”è¦æ±‚ï¼š
        1. å‡†ç¡®å¼•ç”¨æ–‡æ¡£ä¸­çš„å…·ä½“ç« èŠ‚å’Œå†…å®¹
        2. å¦‚æœé—®é¢˜è¶…å‡ºæ–‡æ¡£èŒƒå›´ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
        3. å¯¹äºä¸ç¡®å®šçš„å†…å®¹ï¼Œä¸è¦çŒœæµ‹ï¼Œè¦æ‰¿è®¤ä¸çŸ¥é“
        4. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€æ¸…æ™°
        
        å½“å‰æ–‡æ¡£ï¼šRTCA DO-160G æœºè½½è®¾å¤‡ç¯å¢ƒæ¡ä»¶å’Œè¯•éªŒç¨‹åº"""

    def format_references(self, retrieved_docs: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¼•ç”¨ä¿¡æ¯"""
        references = []
        for i, doc in enumerate(retrieved_docs, 1):
            meta = doc['metadata']
            chapter_info = meta.get('chapter_info', {})
            references.append(
                f"[{i}] æ¥æºï¼š{meta.get('source', 'æœªçŸ¥')}ï¼Œ"
                f"ç« èŠ‚ï¼šç¬¬{chapter_info.get('chapter', 'æœªçŸ¥')}ç«  {chapter_info.get('title', '')}ï¼Œ"
                f"é¡µç ï¼š{meta.get('page', 'æœªçŸ¥')}ï¼Œ"
                f"ç›¸å…³æ€§åˆ†æ•°ï¼š{doc['score']:.3f}"
            )
        return "\n".join(references)

    def build_prompt(self, query: str, retrieved_docs: List[Dict]) -> str:
        """æ„å»ºprompt"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc['content'] for doc in retrieved_docs])
        references = self.format_references(retrieved_docs)

        # æ„å»ºå®Œæ•´prompt
        prompt = f"""{self.system_prompt}

ç›¸å…³å‚è€ƒæ–‡æ¡£ï¼š
{context}

å‚è€ƒæ–‡æ¡£çš„è¯¦ç»†æ¥æºä¿¡æ¯ï¼š
{references}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä»¥ä¸Šå‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ï¼Œå¹¶åœ¨é€‚å½“ä½ç½®å¼•ç”¨æ–‡æ¡£æ¥æºï¼ˆå¦‚[1][2]ï¼‰ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚

å›ç­”ï¼š"""

        return prompt

    def detect_uncertainty(self, response: str) -> bool:
        """æ£€æµ‹å›ç­”ä¸­çš„ä¸ç¡®å®šæ€§"""
        # ç®€å•çš„å…³é”®è¯æ£€æµ‹
        for keyword in self.uncertain_keywords:
            if keyword in response:
                return True

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼•ç”¨
        if not re.search(r'\[\d+\]', response):
            # æ²¡æœ‰å¼•ç”¨å¯èƒ½æ„å‘³ç€ä¸ç¡®å®š
            return True

        return False

    def retrieve_documents(self, query: str) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if self.retriever is None:
            raise ValueError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
        return self.retriever.retrieve(query)

    def answer(self, query: str, conversation_id: str = None) -> Dict:
        """å›ç­”é—®é¢˜"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_documents(query)

        if not retrieved_docs:
            return {
                'answer': "æŠ±æ­‰ï¼Œåœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                'references': [],
                'confidence': 0.0,
                'uncertain': True
            }

        # æ„å»ºprompt
        prompt = self.build_prompt(query, retrieved_docs)

        # ç”Ÿæˆå›ç­”
        response = self.model_manager.generate(prompt)

        # æå–å¼•ç”¨
        citations = re.findall(r'\[(\d+)\]', response)
        cited_docs = []
        for cite in citations:
            try:
                idx = int(cite) - 1
                if 0 <= idx < len(retrieved_docs):
                    cited_docs.append(retrieved_docs[idx])
            except:
                pass

        # æ£€æµ‹ä¸ç¡®å®šæ€§
        uncertain = self.detect_uncertainty(response)

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæ£€ç´¢åˆ†æ•°ï¼‰
        avg_score = sum(doc['score']
                        for doc in cited_docs) / max(len(cited_docs), 1)
        confidence = min(avg_score * 10, 1.0)  # å½’ä¸€åŒ–åˆ°0-1

        # æ›´æ–°å¯¹è¯å†å²
        if conversation_id:
            self.update_conversation_history(conversation_id, query, response)

        return {
            'answer': response,
            'references': cited_docs,
            'confidence': confidence,
            'uncertain': uncertain,
            'retrieved_docs': retrieved_docs
        }

    def update_conversation_history(self, conv_id: str, query: str, response: str):
        """æ›´æ–°å¯¹è¯å†å²"""
        if conv_id not in self.conversation_history:
            self.conversation_history[conv_id] = []

        self.conversation_history[conv_id].extend([
            {"role": "user", "content": query},
            {"role": "assistant", "content": response}
        ])

        # é™åˆ¶å†å²é•¿åº¦
        if len(self.conversation_history[conv_id]) > 10:
            self.conversation_history[conv_id] = self.conversation_history[conv_id][-10:]


# ==================== 8. å¾®è°ƒæ¨¡å— ====================


class FineTuner:
    """æ¨¡å‹å¾®è°ƒå™¨"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.model = None
        self.tokenizer = None

    def prepare_dataset(self, data_path: str):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        dataset = datasets.load_from_disk(data_path)

        def tokenize_function(examples):
            # æ„å»ºè®­ç»ƒæ ¼å¼
            prompts = []
            for context, question, answer in zip(examples['context'],
                                                 examples['question'],
                                                 examples['answer']):
                prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context}

é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼š{answer}
"""
                prompts.append(prompt)

            return self.tokenizer(prompts, truncation=True, padding="max_length", max_length=512)

        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        return tokenized_dataset

    def train(self, train_dataset, eval_dataset=None):
        """è®­ç»ƒæ¨¡å‹"""
        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)

        # åº”ç”¨LoRA
        lora_config = LoraConfig(
            r=self.config.lora_rank,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=["q_proj", "v_proj"],
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )

        self.model = get_peft_model(self.model, lora_config)

        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./output",
            num_train_epochs=3,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=50 if eval_dataset else None,
            save_strategy="steps",
            save_steps=100,
            learning_rate=2e-4,
            fp16=True,
            push_to_hub=False
        )

        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer
        )

        # å¼€å§‹è®­ç»ƒ
        trainer.train()

        # ä¿å­˜æ¨¡å‹
        trainer.save_model("./fine_tuned_model")
        self.tokenizer.save_pretrained("./fine_tuned_model")

        return trainer


# ==================== 9. Gradio Webç•Œé¢ ====================


class GradioApp:
    """Gradio Webåº”ç”¨"""

    def __init__(self, rag_system: RAGSystem):
        self.rag_system = rag_system
        self.conversations = {}  # conversation_id -> history

    def chat_interface(self, message: str, history: list, conversation_id: str):
        """èŠå¤©ç•Œé¢"""
        if not conversation_id:
            conversation_id = str(int(time.time()))

        if conversation_id not in self.conversations:
            self.conversations[conversation_id] = []

        # è·å–å›ç­”
        result = self.rag_system.answer(message, conversation_id)

        # æ ¼å¼åŒ–å›ç­”
        response = result['answer']
        if result['references']:
            response += "\n\n**å‚è€ƒæ¥æºï¼š**\n"
            for i, ref in enumerate(result['references'], 1):
                meta = ref['metadata']
                response += f"{i}. {meta.get('source', 'æœªçŸ¥')} - ç¬¬{meta.get('page', 'æœªçŸ¥')}é¡µ\n"

        if result['uncertain']:
            response = "âš ï¸ **æ³¨æ„ï¼š** è¿™ä¸ªå›ç­”å¯èƒ½ä¸å®Œå…¨å‡†ç¡®ï¼Œå»ºè®®æ ¸å®å®˜æ–¹æ–‡æ¡£ã€‚\n\n" + response

        # æ›´æ–°å†å²
        self.conversations[conversation_id].append((message, response))

        return "", history + [(message, response)]

    def create_web_app(self):
        """åˆ›å»ºWebåº”ç”¨"""
        with gr.Blocks(title="RTCA DO-160Gä¸“å®¶åŠ©æ‰‹", theme=gr.themes.Soft()) as app:
            gr.Markdown("# ğŸ›©ï¸ RTCA DO-160Gä¸“å®¶åŠ©æ‰‹")
            gr.Markdown("åŸºäºQwen-2.5çš„èˆªç©ºæ ‡å‡†æ–‡æ¡£æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

            with gr.Row():
                with gr.Column(scale=2):
                    chatbot = gr.Chatbot(height=600)
                    msg = gr.Textbox(
                        label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šç¬¬4ç« çš„æ¸©åº¦è¯•éªŒè¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
                        lines=2
                    )
                    with gr.Row():
                        submit_btn = gr.Button("å‘é€", variant="primary")
                        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯")

                    conv_id = gr.Textbox(
                        label="ä¼šè¯IDï¼ˆå¯é€‰ï¼‰",
                        placeholder="ç•™ç©ºå°†åˆ›å»ºæ–°ä¼šè¯",
                        lines=1
                    )

                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“Š ç³»ç»Ÿä¿¡æ¯")
                    confidence_bar = gr.Label("ç½®ä¿¡åº¦: å¾…è®¡ç®—")
                    retrieval_stats = gr.Label("æ£€ç´¢æ–‡æ¡£æ•°: 0")
                    model_info = gr.Label(
                        f"æ¨¡å‹: {self.rag_system.config.model_name}")

                    gr.Markdown("### âš™ï¸ è®¾ç½®")
                    top_k_slider = gr.Slider(
                        minimum=1, maximum=10, value=5, step=1,
                        label="æ£€ç´¢æ–‡æ¡£æ•°é‡"
                    )
                    temp_slider = gr.Slider(
                        minimum=0.1, maximum=1.0, value=0.7, step=0.1,
                        label="ç”Ÿæˆæ¸©åº¦"
                    )

                    gr.Markdown("### ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡")
                    latency_display = gr.Label("å“åº”æ—¶é—´: -")

            # äº‹ä»¶å¤„ç†
            msg.submit(
                self.chat_interface,
                [msg, chatbot, conv_id],
                [msg, chatbot]
            )

            submit_btn.click(
                self.chat_interface,
                [msg, chatbot, conv_id],
                [msg, chatbot]
            )

            clear_btn.click(lambda: None, None, chatbot, queue=False)

            # æ›´æ–°è®¾ç½®
            def update_settings(top_k, temperature):
                self.rag_system.config.top_k = int(top_k)
                self.rag_system.config.temperature = temperature
                return "è®¾ç½®å·²æ›´æ–°"

            top_k_slider.change(
                update_settings, [top_k_slider, temp_slider], [])
            temp_slider.change(update_settings, [
                               top_k_slider, temp_slider], [])

        return app


# ==================== 10. FastAPIåç«¯ ====================


class QueryRequest(BaseModel):
    query: str
    conversation_id: Optional[str] = None
    top_k: Optional[int] = None
    temperature: Optional[float] = None


class QueryResponse(BaseModel):
    answer: str
    references: List[Dict]
    confidence: float
    uncertain: bool
    latency: float


class FastAPIApp:
    """FastAPIåç«¯åº”ç”¨"""

    def __init__(self, rag_system: RAGSystem):
        self.rag_system = rag_system
        self.app = FastAPI(title="RAG API", version="1.0.0")
        self.setup_routes()

    def setup_routes(self):
        """è®¾ç½®è·¯ç”±"""

        @self.app.get("/")
        async def root():
            return {"message": "RAG API Service", "status": "running"}

        @self.app.post("/query", response_model=QueryResponse)
        async def query(request: QueryRequest):
            start_time = time.time()

            # ä¸´æ—¶æ›´æ–°é…ç½®
            if request.top_k:
                self.rag_system.config.top_k = request.top_k
            if request.temperature:
                self.rag_system.config.temperature = request.temperature

            # è·å–å›ç­”
            result = self.rag_system.answer(
                request.query, request.conversation_id)

            latency = time.time() - start_time

            return QueryResponse(
                answer=result['answer'],
                references=result['references'],
                confidence=result['confidence'],
                uncertain=result['uncertain'],
                latency=latency
            )

        @self.app.get("/health")
        async def health_check():
            return {"status": "healthy", "timestamp": time.time()}

    def run(self, host: str = "0.0.0.0", port: int = 8000):
        """è¿è¡ŒAPIæœåŠ¡"""
        uvicorn.run(self.app, host=host, port=port)


# ==================== 11. è¯„ä¼°æ¨¡å— ====================


class Evaluator:
    """ç³»ç»Ÿè¯„ä¼°å™¨"""

    def __init__(self):
        self.rouge = evaluate.load('rouge')
        self.bleu = evaluate.load('bleu')

    def evaluate_rag(self, predictions: List[str], references: List[str]) -> Dict:
        """è¯„ä¼°RAGç³»ç»Ÿ"""
        # ROUGEåˆ†æ•°
        rouge_results = self.rouge.compute(
            predictions=predictions,
            references=references,
            use_stemmer=True
        )

        # BLEUåˆ†æ•°
        bleu_results = self.bleu.compute(
            predictions=predictions,
            references=[[ref] for ref in references]
        )

        # å¹»è§‰æ£€æµ‹ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        hallucination_rate = self.detect_hallucinations(
            predictions, references)

        return {
            'rouge': rouge_results,
            'bleu': bleu_results['bleu'],
            'hallucination_rate': hallucination_rate
        }

    def detect_hallucinations(self, predictions: List[str], references: List[str]) -> float:
        """æ£€æµ‹å¹»è§‰ç‡"""
        hallucination_count = 0
        for pred, ref in zip(predictions, references):
            # ç®€å•çš„æ£€æµ‹ï¼šå¦‚æœé¢„æµ‹åŒ…å«å¤§é‡ä¸åœ¨å‚è€ƒä¸­çš„å®ä½“
            pred_entities = set(re.findall(
                r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', pred))
            ref_entities = set(re.findall(
                r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', ref))

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤§é‡æœªåœ¨å‚è€ƒä¸­å‡ºç°çš„å®ä½“
            novel_entities = pred_entities - ref_entities
            if len(novel_entities) > len(pred_entities) * 0.3:  # 30%çš„æ–°å®ä½“
                hallucination_count += 1

        return hallucination_count / len(predictions) if predictions else 0

    def evaluate_citation(self, predictions: List[str], gold_citations: List[List[str]]) -> Dict:
        """è¯„ä¼°å¼•ç”¨è´¨é‡"""
        precisions = []
        recalls = []

        for pred, gold in zip(predictions, gold_citations):
            # æå–é¢„æµ‹ä¸­çš„å¼•ç”¨
            pred_citations = re.findall(r'\[(\d+)\]', pred)

            if not gold:  # å¦‚æœæ²¡æœ‰é‡‘æ ‡å‡†å¼•ç”¨
                if not pred_citations:
                    precisions.append(1.0)
                    recalls.append(1.0)
                else:
                    precisions.append(0.0)
                    recalls.append(0.0)
            else:
                # è®¡ç®—ç²¾åº¦å’Œå¬å›ç‡
                correct = len(set(pred_citations) & set(gold))
                precision = correct / \
                    len(pred_citations) if pred_citations else 0
                recall = correct / len(gold) if gold else 0

                precisions.append(precision)
                recalls.append(recall)

        avg_precision = sum(precisions) / len(precisions) if precisions else 0
        avg_recall = sum(recalls) / len(recalls) if recalls else 0
        f1 = 2 * avg_precision * avg_recall / \
            (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0

        return {
            'citation_precision': avg_precision,
            'citation_recall': avg_recall,
            'citation_f1': f1
        }

# ==================== 12. ä¸»ç¨‹åº ====================


def main():
    """ä¸»ç¨‹åº"""
    import argparse

    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿä¸»ç¨‹åº")
    parser.add_argument("--mode", choices=["init", "train", "web", "api", "eval"],
                        default="web", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", default="./config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data", default="./data/rtca_doc.pdf", help="æ–‡æ¡£è·¯å¾„")
    parser.add_argument("--host", default="0.0.0.0", help="APIä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=7860, help="ç«¯å£å·")

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = RAGConfig.load(args.config)

    if args.mode == "init":
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        print("æ­£åœ¨åˆå§‹åŒ–çŸ¥è¯†åº“...")
        processor = DocumentProcessor(config)
        chunks = processor.process_directory(args.data)

        vector_store_manager = VectorStoreManager(config)
        vector_store = vector_store_manager.create_vector_store(chunks)

        print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼Œå…±å¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")

    elif args.mode == "train":
        # è®­ç»ƒæ¨¡å¼
        print("æ­£åœ¨å‡†å¤‡å¾®è°ƒ...")
        fine_tuner = FineTuner(config)

        # åŠ è½½æ•°æ®é›†
        train_dataset = fine_tuner.prepare_dataset(
            config.fine_tune_data_path + "/train")
        eval_dataset = fine_tuner.prepare_dataset(
            config.fine_tune_data_path + "/eval")

        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹è®­ç»ƒ...")
        trainer = fine_tuner.train(train_dataset, eval_dataset)
        print("è®­ç»ƒå®Œæˆ!")

    elif args.mode in ["web", "api"]:
        # è¿è¡ŒæœåŠ¡
        print("æ­£åœ¨åŠ è½½RAGç³»ç»Ÿ...")

        # åŠ è½½å‘é‡æ•°æ®åº“
        vector_store_manager = VectorStoreManager(config)
        vector_store = vector_store_manager.load_vector_store()

        # åŠ è½½æ–‡æ¡£å—ï¼ˆç”¨äºç¨€ç–æ£€ç´¢ï¼‰
        with open(os.path.join(config.knowledge_base_path, "processed_chunks.json"), 'r') as f:
            chunks = json.load(f)

        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = HybridRetriever(vector_store, chunks, config)

        # åˆ›å»ºRAGç³»ç»Ÿ
        rag_system = RAGSystem(config)
        rag_system.retriever = retriever

        if args.mode == "web":
            # å¯åŠ¨Gradio Webç•Œé¢
            print("å¯åŠ¨Gradio Webç•Œé¢...")
            app = GradioApp(rag_system)
            gradio_app = app.create_web_app()
            gradio_app.launch(server_name=args.host, server_port=args.port)
        else:
            # å¯åŠ¨FastAPIæœåŠ¡
            print("å¯åŠ¨FastAPIæœåŠ¡...")
            api_app = FastAPIApp(rag_system)
            api_app.run(host=args.host, port=args.port)

    elif args.mode == "eval":
        # è¯„ä¼°æ¨¡å¼
        print("å¼€å§‹è¯„ä¼°...")
        evaluator = Evaluator()

        # è¿™é‡Œéœ€è¦åŠ è½½æµ‹è¯•æ•°æ®é›†
        # test_data = load_test_data()

        # è¿›è¡Œè¯„ä¼°
        # results = evaluator.evaluate_rag(predictions, references)
        # print(f"è¯„ä¼°ç»“æœ: {results}")

        print("è¯„ä¼°æ¨¡å¼å¾…å®ç°...")


if __name__ == "__main__":
    main()
